{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Local RAG Tutorial\n",
    "\n",
    "A hands-on tutorial for building a **Retrieval-Augmented Generation** system that runs entirely on your machine.\n",
    "\n",
    "Sources: https://culinary.sonoma.edu/sites/culinary/files/2025-03/Pasta%20Carbonara%20Recipe.pdf\n",
    "\n",
    "## What we'll build\n",
    "\n",
    "```\n",
    "Your Question ‚Üí Embed ‚Üí Search Vector DB ‚Üí Retrieve Chunks ‚Üí LLM + Context ‚Üí Answer\n",
    "```\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Ollama** installed and running ([download here](https://ollama.com/download))\n",
    "2. A model pulled: `ollama pull llama3.1:8b`\n",
    "3. Some PDFs in the `./documents` folder\n",
    "\n",
    "\n",
    "# Make sure Ollama is running (in one terminal)\n",
    "Terminal `ollama serve`\n",
    "\n",
    "Check if it's running by checking: http://localhost:11434/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 0: Install Dependencies\n",
    "\n",
    "Run this cell once to install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if you haven't installed the dependencies yet\n",
    "\n",
    "# !pip install llama-index\n",
    "# !pip install llama-index-llms-ollama\n",
    "# !pip install llama-index-embeddings-huggingface\n",
    "# !pip install llama-index-vector-stores-chroma\n",
    "# !pip install chromadb\n",
    "# !pip install gradio\n",
    "# !pip install pypdf\n",
    "# !pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Configuration\n",
    "\n",
    "Set up paths and model choices. **Edit these to match your setup!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION - Adjust these for your setup\n",
    "# =============================================================================\n",
    "import os\n",
    "\n",
    "# Folder containing your PDF documents\n",
    "DOCUMENTS_FOLDER = \"./documents\"\n",
    "\n",
    "# Folder for the vector database (persists embeddings between runs)\n",
    "CHROMA_DB_FOLDER = \"./chroma_db\"\n",
    "\n",
    "# Create folders if they don't exist\n",
    "os.makedirs(DOCUMENTS_FOLDER, exist_ok=True)\n",
    "os.makedirs(CHROMA_DB_FOLDER, exist_ok=True)\n",
    "\n",
    "# Ollama LLM model (must be pulled first: ollama pull llama3.1:8b)\n",
    "LLM_MODEL = \"llama3.1:8b\"  # Alternatives: \"mistral\", \"phi3\", \"gemma2\"\n",
    "\n",
    "# HuggingFace embedding model (downloaded automatically)\n",
    "# This replaces the need for nomic-embed-text in Ollama!\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # Fast, 384 dimensions\n",
    "# Alternatives:\n",
    "# - \"sentence-transformers/all-mpnet-base-v2\"  (better quality, slower)\n",
    "# - \"BAAI/bge-small-en-v1.5\"  (good balance)\n",
    "# - \"BAAI/bge-large-en-v1.5\"  (best quality, needs more RAM)\n",
    "\n",
    "# Chunking settings\n",
    "CHUNK_SIZE = 512      # Characters per chunk\n",
    "CHUNK_OVERLAP = 50    # Overlap between chunks\n",
    "\n",
    "# Retrieval settings\n",
    "TOP_K = 3  # Number of chunks to retrieve per query\n",
    "\n",
    "print(f\"‚úÖ Folders ready: {DOCUMENTS_FOLDER}, {CHROMA_DB_FOLDER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Import Libraries\n",
    "\n",
    "We're using:\n",
    "- **LlamaIndex**: Orchestrates the RAG pipeline\n",
    "- **Ollama**: Runs the LLM locally\n",
    "- **HuggingFace**: Provides the embedding model (no extra downloads needed!)\n",
    "- **ChromaDB**: Stores vectors locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# LlamaIndex core\n",
    "from llama_index.core import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Settings,\n",
    "    StorageContext,\n",
    ")\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# Local LLM via Ollama\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# HuggingFace embeddings (no Ollama embedding model needed!)\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Vector store\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Initialize the LLM\n",
    "\n",
    "We use **Ollama** to run a local LLM. This is the model that generates answers.\n",
    "\n",
    "Make sure Ollama is running! In a terminal: `ollama serve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üîß Connecting to Ollama with model: {LLM_MODEL}\")\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = Ollama(\n",
    "    model=LLM_MODEL,\n",
    "    request_timeout=120.0,  # Local models can be slow without GPU\n",
    ")\n",
    "\n",
    "# Quick test to make sure it's working\n",
    "try:\n",
    "    test = llm.complete(\"Say 'hello' in one word.\")\n",
    "    print(f\"‚úÖ Ollama is working! Test response: {test}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error connecting to Ollama: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Is Ollama running? Start it with: ollama serve\")\n",
    "    print(f\"  2. Is the model pulled? Run: ollama pull {LLM_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Initialize the Embedding Model\n",
    "\n",
    "We use **HuggingFace sentence-transformers**.\n",
    "\n",
    "Benefits:\n",
    "- Downloads automatically (no manual `ollama pull`)\n",
    "- Wide variety of models available\n",
    "- Well-documented and widely used\n",
    "\n",
    "The embedding model converts text into vectors (lists of numbers) that capture semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"üîß Loading embedding model: {EMBEDDING_MODEL}\")\n",
    "print(\"   (This may download the model on first run - ~90MB for MiniLM)\")\n",
    "\n",
    "# Initialize the embedding model from HuggingFace\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    # Uncomment below if you have a GPU and want faster embeddings\n",
    "    # device=\"cuda\",\n",
    ")\n",
    "\n",
    "# Quick test\n",
    "test_embedding = embed_model.get_text_embedding(\"Hello world\")\n",
    "print(f\"‚úÖ Embedding model loaded!\")\n",
    "print(f\"   Vector dimensions: {len(test_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Configure LlamaIndex Settings\n",
    "\n",
    "We tell LlamaIndex which models to use and how to chunk documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the LLM and embedding model as defaults\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Configure document chunking\n",
    "Settings.node_parser = SentenceSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Settings configured:\")\n",
    "print(f\"   LLM: {LLM_MODEL}\")\n",
    "print(f\"   Embeddings: {EMBEDDING_MODEL}\")\n",
    "print(f\"   Chunk size: {CHUNK_SIZE} chars\")\n",
    "print(f\"   Chunk overlap: {CHUNK_OVERLAP} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Set Up ChromaDB Vector Store\n",
    "\n",
    "**ChromaDB** stores our document embeddings locally. It persists to disk, so you don't need to re-embed documents every time.\n",
    "\n",
    "Think of it as a database optimized for finding similar vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Initializing ChromaDB vector store...\")\n",
    "\n",
    "# Create the database folder if it doesn't exist\n",
    "os.makedirs(CHROMA_DB_FOLDER, exist_ok=True)\n",
    "\n",
    "# Create a persistent ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_FOLDER)\n",
    "\n",
    "# Get or create a collection for our documents\n",
    "chroma_collection = chroma_client.get_or_create_collection(\n",
    "    name=\"tutorial_documents\",\n",
    "    metadata={\"description\": \"RAG tutorial document collection\"}\n",
    ")\n",
    "\n",
    "# Wrap for LlamaIndex\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "print(f\"‚úÖ Vector store ready at: {CHROMA_DB_FOLDER}\")\n",
    "print(f\"   Existing documents in collection: {chroma_collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 7: Load and Index Documents\n",
    "\n",
    "This is where the magic happens:\n",
    "\n",
    "1. **Load** PDFs from the documents folder\n",
    "2. **Chunk** them into smaller pieces\n",
    "3. **Embed** each chunk into a vector\n",
    "4. **Store** vectors in ChromaDB\n",
    "\n",
    "‚ö†Ô∏è **Make sure you have PDFs in the `./documents` folder!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create documents folder if it doesn't exist\n",
    "os.makedirs(DOCUMENTS_FOLDER, exist_ok=True)\n",
    "\n",
    "# Check for PDFs\n",
    "pdf_files = list(Path(DOCUMENTS_FOLDER).glob(\"*.pdf\"))\n",
    "\n",
    "if not pdf_files:\n",
    "    print(f\"‚ö†Ô∏è  No PDFs found in {DOCUMENTS_FOLDER}\")\n",
    "    print(\"\\n   Please add some PDF files and re-run this cell!\")\n",
    "    print(f\"\\n   Example: Copy a PDF to {os.path.abspath(DOCUMENTS_FOLDER)}\")\n",
    "else:\n",
    "    print(f\"üìÑ Found {len(pdf_files)} PDF(s):\")\n",
    "    for pdf in pdf_files:\n",
    "        print(f\"   - {pdf.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and index the documents\n",
    "# This cell may take a few minutes depending on document size\n",
    "\n",
    "print(\"üìñ Loading documents...\")\n",
    "documents = SimpleDirectoryReader(\n",
    "    input_dir=DOCUMENTS_FOLDER,\n",
    "    required_exts=[\".pdf\"],\n",
    ").load_data()\n",
    "\n",
    "print(f\"   Loaded {len(documents)} document sections\")\n",
    "\n",
    "print(\"\\nüî¢ Creating embeddings and building index...\")\n",
    "print(\"   (This may take a few minutes the first time)\")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Index built successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 8: Create the Query Engine\n",
    "\n",
    "The query engine ties everything together:\n",
    "1. Takes your question\n",
    "2. Embeds it\n",
    "3. Finds similar chunks in the vector store\n",
    "4. Sends chunks + question to the LLM\n",
    "5. Returns the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the query engine\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=TOP_K,  # How many chunks to retrieve\n",
    "    response_mode=\"compact\",  # Combine chunks into coherent response\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Query engine ready!\")\n",
    "print(f\"   Will retrieve top {TOP_K} chunks for each query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 9: Query Function with Source Display\n",
    "\n",
    "This function shows both the answer AND the retrieved chunks, so you can see what context the LLM used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "def ask(question: str, show_sources: bool = True):\n",
    "    \"\"\"\n",
    "    Ask a question and display the answer with sources.\n",
    "    \n",
    "    Args:\n",
    "        question: Your question about the documents\n",
    "        show_sources: Whether to display retrieved chunks\n",
    "    \"\"\"\n",
    "    print(f\"üîç Question: {question}\\n\")\n",
    "    print(\"‚è≥ Thinking...\")\n",
    "    \n",
    "    # Query the RAG system\n",
    "    response = query_engine.query(question)\n",
    "    \n",
    "    # Display the answer\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    display(Markdown(f\"## üí¨ Answer\\n\\n{response}\"))\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Display retrieved sources\n",
    "    if show_sources:\n",
    "        print(\"\\nüìö Retrieved Chunks:\\n\")\n",
    "        \n",
    "        for i, node in enumerate(response.source_nodes, 1):\n",
    "            filename = node.metadata.get(\"file_name\", \"Unknown\")\n",
    "            page = node.metadata.get(\"page_label\", \"?\")\n",
    "            score = node.score if node.score else 0\n",
    "            \n",
    "            print(f\"--- Chunk {i} (Score: {score:.3f}) ---\")\n",
    "            print(f\"Source: {filename}, Page {page}\")\n",
    "            print(f\"\\n{node.text[:400]}{'...' if len(node.text) > 400 else ''}\")\n",
    "            print()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéØ Try It Out!\n",
    "\n",
    "Now you can ask questions about your documents. Edit the question below and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question about your document(s)!\n",
    "\n",
    "response = ask(\"What is the main topic of this document?\", show_sources=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try another question\n",
    "\n",
    "response = ask(\"What methodology or approach is described?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try asking something NOT in the documents to see how it handles it\n",
    "\n",
    "response = ask(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üß™ Experiments to Try\n",
    "\n",
    "Use the cells below to explore how RAG behaves in different situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Specificity\n",
    "\n",
    "Compare vague vs. specific questions. How does retrieval quality change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vague question\n",
    "response = ask(\"What is this about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific question\n",
    "response = ask(\"What specific methods or techniques are used in section 3?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Attribution Prompting\n",
    "\n",
    "Can you get the model to cite its sources more explicitly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without attribution request\n",
    "response = ask(\"What are the main findings?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With attribution request\n",
    "response = ask(\"What are the main findings? Quote the relevant passages and cite page numbers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3: Changing Retrieval Settings\n",
    "\n",
    "What happens if we retrieve more or fewer chunks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query engine that retrieves MORE chunks\n",
    "query_engine_more = index.as_query_engine(similarity_top_k=5)\n",
    "\n",
    "print(\"Retrieving 5 chunks instead of 3:\\n\")\n",
    "response = query_engine_more.query(\"Summarize the key points.\")\n",
    "print(response)\n",
    "print(f\"\\nUsed {len(response.source_nodes)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a query engine that retrieves FEWER chunks\n",
    "query_engine_less = index.as_query_engine(similarity_top_k=1)\n",
    "\n",
    "print(\"Retrieving only 1 chunk:\\n\")\n",
    "response = query_engine_less.query(\"Summarize the key points.\")\n",
    "print(response)\n",
    "print(f\"\\nUsed {len(response.source_nodes)} chunk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üìù Reflection Questions\n",
    "\n",
    "After experimenting, consider:\n",
    "\n",
    "1. **What made the difference between good and bad RAG responses?**\n",
    "\n",
    "2. **When did the system fail to use the context properly?**\n",
    "\n",
    "3. **How might you evaluate RAG quality systematically?**\n",
    "\n",
    "4. **What would you change about the chunking strategy?**\n",
    "\n",
    "5. **When might RAG NOT be the right approach?**\n",
    "\n",
    "6. **What happens when you have contradicting documents in your folder?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö References & Further Reading\n",
    "\n",
    "### Foundational Papers\n",
    "- **RAG**: Lewis et al. (2020). [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401). *NeurIPS 2020*.\n",
    "- **Dense Retrieval**: Karpukhin et al. (2020). [Dense Passage Retrieval for Open-Domain Question Answering](https://arxiv.org/abs/2004.04906). *EMNLP 2020*.\n",
    "- **Sentence Embeddings**: Reimers & Gurevych (2019). [Sentence-BERT](https://arxiv.org/abs/1908.10084). *EMNLP 2019*.\n",
    "\n",
    "### Why RAG Works This Way\n",
    "- Liu et al. (2023). [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172). *Explains why we chunk and retrieve rather than stuffing everything in context.*\n",
    "\n",
    "### Tools We Used\n",
    "- [LlamaIndex Documentation](https://docs.llamaindex.ai/)\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [Ollama](https://ollama.com/) | [Model Library](https://ollama.com/library)\n",
    "- [Sentence Transformers](https://www.sbert.net/) | [Model Hub](https://huggingface.co/sentence-transformers)\n",
    "\n",
    "### Going Deeper\n",
    "- Gao et al. (2024). [RAG Survey](https://arxiv.org/abs/2312.10997). *Comprehensive overview of RAG techniques.*\n",
    "- [Anthropic RAG Guide](https://docs.anthropic.com/en/docs/build-with-claude/retrieval-augmented-generation)\n",
    "- [RAGAS Evaluation Framework](https://github.com/explodinggradients/ragas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Appendix: Understanding the Components\n",
    "\n",
    "### Embedding Models Comparison\n",
    "\n",
    "| Model | Dimensions | Speed | Quality | Size |\n",
    "|-------|------------|-------|---------|------|\n",
    "| all-MiniLM-L6-v2 | 384 | ‚ö° Fast | Good | ~90MB |\n",
    "| all-mpnet-base-v2 | 768 | Medium | Better | ~420MB |\n",
    "| bge-small-en-v1.5 | 384 | ‚ö° Fast | Good | ~130MB |\n",
    "| bge-large-en-v1.5 | 1024 | Slow | Best | ~1.3GB |\n",
    "\n",
    "### Chunking Strategies\n",
    "\n",
    "- **Smaller chunks** (256-512): More precise retrieval, but may lose context\n",
    "- **Larger chunks** (1024+): More context, but may retrieve irrelevant content\n",
    "- **Overlap**: Helps prevent cutting sentences in the middle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
